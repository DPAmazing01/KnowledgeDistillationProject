The Knowledge Distillation Project is a project that focuses on exploring response-based KD using offline distillation using the original
and compressed versions of the VGG16, ResNet18 and DenseNet121 models (Project was done in a team of 2, where my main contributions were writing
the KD implementation and optimizing the training used with the VGG16 and ResNet18 models)

Checkpoint Folder Files Link:
https://drive.google.com/drive/folders/1WcV5V3d7Uabt1EgWOWi5c_TLVVscvIaY?usp=sharing
